{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start simple, Start linearly\n",
    "\n",
    "The program below will mark the first RL algorithm truly implemented in this project. As such, to see if I've got the basics down, I started with a simply linear policy to learn. The algorithm contains two main parts. \n",
    "\n",
    "* The first is to learn a surrogate dynamics model of the environment. I personally like to think of this as the agent's belief of how the environment behaves, so the term surrogate doesn't too appropriate. A GP model will be used, with a gaussian covariance kernel. \n",
    "\n",
    "* The second is to use this belief of the system to learn an optimal policy. As mentioned previously, a linear policy structure will be used. Furthermore, bayesian optimisation with expected improvement will be used to find the optimal policy values. \n",
    "\n",
    "Let's see how this goes. Fingers crossed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import GPy\n",
    "import GPyOpt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "import gympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the characteristics of the particular exercise. In this case, for example, the type of learning analysis will be 'Reset' since the initial state of the environment is reset for each test point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysisType_learning = 'Reset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the gym environment and create the array definitions for  input, output and predictions. Also generate initial values for the relevant variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the environment: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-14 00:29:36,810] Making new env: Pendulum-v0\n"
     ]
    }
   ],
   "source": [
    "env, observations, inputsNumpy, observationsNumpy, rewardsNumpy, action, predictionPDF = gympy.setupEnvironment(defaultEnvironment = 'Pendulum-v0')\n",
    "k_dynamics = GPy.kern.RBF(input_dim=4, variance=1., lengthscale=1.)\n",
    "k_rewards = GPy.kern.RBF(input_dim=3, variance=1., lengthscale=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initial learning phase - learn surrogate model. This will be done with 500 random training data points. As such a loop will first be initiated to generate the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for attempt in range(100):\n",
    "    [inputsNumpy, buf5erInput, actionToTake] = gympy.appendInputArray(inputsNumpy, action, observations, attempt)\n",
    "    [observations, rewards, done, info] = env.step(actionToTake)\n",
    "    [observationsNumpy, bufferObservations] = gympy.appendObservationsArray(observationsNumpy, observations, attempt)\n",
    "    [rewardsNumpy, bufferRewards] = gympy.appendRewardsArray(rewardsNumpy, rewards, attempt)\n",
    "    observations = env.reset()\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the training points to generate the training points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-14 00:29:44,692] initializing Y\n",
      "[2016-11-14 00:29:44,693] initializing inference method\n",
      "[2016-11-14 00:29:44,693] adding kernel and likelihood as parameters\n"
     ]
    }
   ],
   "source": [
    "m_dynamics = gympy.generateModel(inputsNumpy, observationsNumpy, k_dynamics)\n",
    "m_dynamics.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once the learning phase has been completed, it is necessary to define the goal state (that is the terminal state) and how the reward can be calculated in terms of the current state with respect to the goal state. \n",
    "\n",
    "In this case, the reward is calculated as the sum of the geometric distances between the each of the current states and the goal states. Please note that since each of these will be $ \\geqslant \\: 0$, it is technically speaking a cost, rather than a reward. However, since the difference betweeen the two is merely a negative symbol, reward and cost will be used interchangebly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goalState = np.array([1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateTotalReward(predictionPDF, goalState, totalReward):\n",
    "    currentReward = 100*np.sum(np.sqrt((predictionPDF*predictionPDF)+ (goalState*goalState)))\n",
    "    totalReward = totalReward + currentReward\n",
    "    \n",
    "    return totalReward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy must now be defined in terms of its parameters. In this particular example, a linear policy was chosen, for simplicity. This will be of the form:\n",
    "\n",
    "$$ \\pi(s_t) = \\phi(s_t)^T \\mathbf{\\theta},$$\n",
    "\n",
    "where the basis functions in $\\phi$ are simply the state variables, and the vector $\\theta$ represents the weights. Thus the action will be selected by policy that looks like:\n",
    "\n",
    "$$ a_t = s_{t,1}\\theta_1 + s_{t,2}\\theta_2  +  s_{t,3}\\theta_3, $$\n",
    "\n",
    "since this particular problem has 3 state variables. The reinforcement learning problem now becomes one of finiding the values of $\\theta$ such that the cost defined earlier is minimised. \n",
    "\n",
    "The first step of this is to generate random initial values for the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policyParameters =  np.random.uniform(-1., 1., ([1, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a function is defined to return the action that needs to be taken as given by the policy defined by the current parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def action_fromPolicy(policyParameters, bufferObservations_policy):\n",
    "    actionToTake_policy = 0\n",
    "    [r,c] = policyParameters.shape\n",
    "    for i in range(c):\n",
    "        actionToTake_policy = actionToTake_policy + (bufferObservations_policy[0,i] * policyParameters[0,i])\n",
    "\n",
    "    return actionToTake_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the policy parameters that minimise the total reward is clearly an optimisation problem. As such the optimisation method that will be used for this particular notebook is Bayesian Optimisation using Expected Improvement. Simply put, this methods chooses the next test point based on the highest expected improvement from the current minimun point (for a minimising problem that is). Thusly, a good mix of exploration and exploitation can be utilised to find the global minimum given the state box.\n",
    "\n",
    "The python library GPyOpt will be utilised for this. This requires the definition of an objective function; this is function that is to be optimised. In the present work, this will be the mapping from the policyy parameters to the total reward. Since this is a model based RL algorithm, the GP model that was trained previously will be used to generate the total reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objectiveFunction(policyParameters):\n",
    "    bufferObservations_policy = np.random.uniform(-1., 1., ([1, 3]))\n",
    "    bufferInpust_policy = np.zeros((1,4))\n",
    "    totalIterations = 100\n",
    "    \n",
    "    totalReward = 0\n",
    "    \n",
    "    goalState = np.array([-1,0,0])\n",
    "    \n",
    "    for i in range(totalIterations):\n",
    "        actionToTake_policy = action_fromPolicy(policyParameters, bufferObservations_policy)\n",
    "        bufferActionToTake = np.array([actionToTake_policy])\n",
    "        bufferActionToTake = np.reshape(bufferActionToTake, ([1,1]))\n",
    "        bufferInputs_policy = np.append(bufferObservations_policy, bufferActionToTake, axis = 1) \n",
    "\n",
    "        predictionPDF = m_dynamics.predict(bufferInputs_policy)[0]\n",
    "                \n",
    "        totalReward = generateTotalReward(predictionPDF, goalState, totalReward)\n",
    "        \n",
    "    return totalReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bounds = [{'domain': (-1,1), 'name': 'var_1', 'type': 'continuous', 'dimensionality':3}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-14 00:41:20,834] initializing Y\n",
      "[2016-11-14 00:41:20,835] initializing inference method\n",
      "[2016-11-14 00:41:20,836] adding kernel and likelihood as parameters\n"
     ]
    }
   ],
   "source": [
    "myBopt = GPyOpt.methods.BayesianOptimization(f = objectiveFunction, domain = bounds, acquisition_type ='EI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_iter = 200               # evaluation budget\n",
    "myBopt.run_optimization(max_iter)   # run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "policyParameters_optimal = myBopt.X[np.argmin(myBopt.Y)]\n",
    "policyParameters_optimal = np.reshape(policyParameters_optimal, ([1,3]))\n",
    "print policyParameters_optimal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-14 00:55:48,874] Making new env: Pendulum-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "observations = env.reset()\n",
    "for i in range (2000):\n",
    "    env.render()\n",
    "    observations = np.reshape(observations, ([1,3]))\n",
    "    action = action_fromPolicy(policyParameters_optimal, observations)\n",
    "    action = np.array([action])\n",
    "    observations, rewards, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
